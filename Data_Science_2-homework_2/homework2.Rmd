---
title: "homework2"
author: "Zsombor Hegedus"
date: '2021 április 6 '
output: html_document
---

``` {r, include = F}
library(keras)
library(grid)
library(magick)
library(tidyverse)
library(filesstrings)
root_folder <- ('C:/Workspace/R_directory/CEU/machine-learning-course/data/hot-dog-not-hot-dog')




```

## Hotdog or not hotdog 

In this section I am going to present my solution for an image classification task in which I will build a model to identify hotdogs. First of all I stripped out randomly 50 hotdog and non-hotdogs to use that as validation (an R script in my repo has the code that does this). After that I loaded up images in 150x150 size with keras's inbuilt data generator functions. 

``` {r preprocess1, include = F}
train_datagen <- image_data_generator(rescale = 1/255)  
valid_datagen <- image_data_generator(rescale = 1/255)  
test_datagen <- image_data_generator(rescale = 1/255) 


image_size <- c(150, 150)
batch_size <- 50

train_generator <- flow_images_from_directory(
    file.path(root_folder,'train'), # Target directory  
    train_datagen,              # Data generator
    target_size = image_size,  # Resizes all images to 150 × 150
    batch_size = batch_size,
    class_mode = "binary"       # binary_crossentropy loss for binary labels
)

valid_generator <- flow_images_from_directory(
    file.path(root_folder,'/validation/'), # Target directory  
    valid_datagen,              # Data generator
    target_size = image_size,  # Resizes all images to 150 × 150
    batch_size = batch_size,
    class_mode = "binary"       # binary_crossentropy loss for binary labels
)


test_generator <- flow_images_from_directory(
    file.path(root_folder,'/test/'),
    test_datagen,
    target_size = image_size,
    batch_size = batch_size,
    class_mode = "binary"
)

```

### Baseline model

I first I used a convolutional network very similar to what was available in the course repo. As seen below it has thee 2d convolutional layers with max pooling, then a dropout layer with flattener, and two dense layers in the end, where the last one - the output layer - is with sigmoid activation function. The output layer has only one node in the end, since all we care about is whether the image is a hotdog or not. 

``` {r, include = F}
hotdog_model <- keras_model_sequential() 

hotdog_model %>% 
    layer_conv_2d(filters = 32,
                  kernel_size = c(3, 3), 
                  activation = 'relu',
                  input_shape = c(150, 150, 3)) %>%
    layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
    layer_conv_2d(filters = 16,
                  kernel_size = c(3, 3), 
                  activation = 'relu') %>%
    layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
    layer_conv_2d(filters = 16,
                  kernel_size = c(3, 3), 
                  activation = 'relu') %>%
    layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
    layer_dropout(rate = 0.25) %>% 
    layer_flatten() %>% 
    layer_dense(units = 8, activation = 'relu') %>% 
    layer_dense(units = 1, activation = "sigmoid")


hotdog_model %>% compile(
    loss = "binary_crossentropy",
    optimizer = 'rmsprop',
    metrics = c("accuracy")
)

history <- hotdog_model %>% fit(
    train_generator,
    steps_per_epoch = 398/batch_size,
    epochs = 5,
    validation_data = valid_generator,
    validation_steps = 1
)

hotdog_acc<-hotdog_model %>% evaluate(test_generator)

```

```{r, fig.width=10,fig.height=6, fig.align='center', echo = FALSE , results = "asis", warning = FALSE, message = FALSE}

plot(history) + theme_bw()
```

The history shows that validation accuracy will not be very high - it will be in the 60-70% accuracy range, but this is understandable as we are dealing with real images, and not preprocessed cleaned ones like in the case of fashion minst. I will further explore whether this accuracy can get higher with a few tweaks.

I first experimented with image augmentation. For that I did a few preprocessing steps in the train generator functions - I shifted the images and zoomed in on them as well. The layers were the same as in the earlier example, so let's see how accuracy faired. 

```{r ,include = F}
train_datagen = image_data_generator(
    rescale = 1/255,
    rotation_range = 40,
    width_shift_range = 0.2,
    height_shift_range = 0.2,
    shear_range = 0.2,
    zoom_range = 0.2, # crops corners of most photos --> useful!
    horizontal_flip = TRUE,
    fill_mode = "nearest"
)

train_generator <- flow_images_from_directory(
    file.path(root_folder,'train'), # Target directory  
    train_datagen,              # Data generator
    target_size = image_size,  # Resizes all images to 150 × 150
    batch_size = batch_size,
    class_mode = "binary"       # binary_crossentropy loss for binary labels
)

hotdog_aug_model <- keras_model_sequential() 

hotdog_aug_model %>% 
    layer_conv_2d(filters = 32,
                  kernel_size = c(3, 3), 
                  activation = 'relu',
                  input_shape = c(150, 150, 3)) %>%
    layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
    layer_conv_2d(filters = 16,
                  kernel_size = c(3, 3), 
                  activation = 'relu') %>%
    layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
    layer_conv_2d(filters = 16,
                  kernel_size = c(3, 3), 
                  activation = 'relu') %>%
    layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
    layer_dropout(rate = 0.25) %>% 
    layer_flatten() %>% 
    layer_dense(units = 8, activation = 'relu') %>% 
    layer_dense(units = 1, activation = "sigmoid")


hotdog_aug_model %>% compile(
    loss = "binary_crossentropy",
    optimizer = 'rmsprop',
    metrics = c("accuracy")
)

history2 <- hotdog_aug_model %>% fit(
    train_generator,
    steps_per_epoch = 398/batch_size,
    epochs = 5,
    validation_data = valid_generator,
    validation_steps = 1
)

hotdog_aug_acc<- hotdog_aug_model %>% evaluate(test_generator)
```

```{r, fig.width=10,fig.height=6, fig.align='center', echo = FALSE , results = "asis", warning = FALSE, message = FALSE}

plot(history2) + theme_bw()
```

There is a minor improvement in accuracy, so it is worth playing around with data augumentation for this task. 

Lastly I tried out tansfer learning - the idea of transfer learning is to use pre-trained models which have been fed with hundreds of thousands of images and use the knowedge gained by their learning to predict for my particular problem as well. I will use the ImageNet image database that is available for free and has processed a huge amount of data already. 

``` {r, include = F}
train_datagen = image_data_generator(
    rescale = 1/255,
    rotation_range = 40,
    width_shift_range = 0.2,
    height_shift_range = 0.2,
    shear_range = 0.2,
    zoom_range = 0.2,
    horizontal_flip = TRUE,
    fill_mode = "nearest"
)



image_size <- c(128, 128)
batch_size <- 100  # for speed up

train_generator <- flow_images_from_directory(
    file.path(root_folder,'train'), # Target directory  
    train_datagen,              # Data generator
    target_size = image_size,  # Resizes all images to 150 × 150
    batch_size = batch_size,
    class_mode = "binary"       # binary_crossentropy loss for binary labels
)

valid_generator <- flow_images_from_directory(
    file.path(root_folder,'/validation/'), # Target directory  
    valid_datagen,              # Data generator
    target_size = image_size,  # Resizes all images to 150 × 150
    batch_size = batch_size,
    class_mode = "binary"       # binary_crossentropy loss for binary labels
)


test_generator <- flow_images_from_directory(
    file.path(root_folder,'/test/'),
    test_datagen,
    target_size = image_size,
    batch_size = batch_size,
    class_mode = "binary"
)


# create the base pre-trained model
imagenet <- application_mobilenet(weights = 'imagenet', include_top = FALSE,
                                    input_shape = c(image_size, 3))
# freeze all convolutional mobilenet layers
freeze_weights(imagenet)

# train only the top layers (which were randomly initialized)

# add our custom layers
predictions <- imagenet$output %>% 
    layer_global_average_pooling_2d() %>% 
    layer_dense(units = 16, activation = 'relu') %>% 
    layer_dense(units = 1, activation = 'sigmoid')

# this is the model we will train
model <- keras_model(inputs = imagenet$input, outputs = predictions)

# compile the model (should be done *after* setting layers to non-trainable)
model %>% compile(
    loss = "binary_crossentropy",
    optimizer = optimizer_rmsprop(lr = 2e-5),
    metrics = c("accuracy")
)


history3<- model %>% fit(
    train_generator,
    steps_per_epoch = 398 / batch_size,
    epochs = 3,  # takes long time to train more
    validation_data = valid_generator,
    validation_steps = 1
)

transfer_acc<- model %>% evaluate(test_generator)
```

```{r, fig.width=10,fig.height=6, fig.align='center', echo = FALSE , results = "asis", warning = FALSE, message = FALSE}

plot(history3) + theme_bw()
```


```{r , echo = FALSE , results = "asis", warning = FALSE, message = FALSE }


summary3 <- cbind(hotdog_acc, hotdog_aug_acc, transfer_acc)  %>% as.data.frame()
colnames(summary3) <- c('Baseline model', 'Baseline model + Augmentation', 'Imagenet Transfer learning model')
 
knitr::kable(summary3)
```

All in all my models were not very accurate on the test set, but there was some tangible imrpovement coming from a few tweaks that I experimented with in this task. 